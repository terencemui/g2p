{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terencemui/Documents/projects/g2p/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jiwer import wer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "log_dir = \"logs/run_3\"\n",
    "writer = SummaryWriter(log_dir=log_dir, purge_step=0)\n",
    "\n",
    "batch_size = 16\n",
    "lr = 5e-5\n",
    "\n",
    "model_name = \"google-t5/t5-small\"\n",
    "model_path = \"g2p_t5_model\"\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/terencemui/Documents/projects/g2p/.venv/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "writer.add_scalar(\"Learning Rate\", lr)\n",
    "writer.add_scalar(\"Batch Size\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: clean_data/train-clean-100.csv\n",
      "Original dataset size:\t28538\n",
      "Reduced dataset size:\t28536\n",
      "Dataset: clean_data/test-clean.csv\n",
      "Original dataset size:\t2620\n",
      "Reduced dataset size:\t2598\n"
     ]
    }
   ],
   "source": [
    "# Function to force character-level tokenization\n",
    "def format_input(text):\n",
    "    return f\"grapheme to phoneme: {text}\"\n",
    "    # return f\"grapheme to phoneme: {\" \".join(text)}\"\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class G2PDataset(Dataset):\n",
    "    def __init__(self, file_path, max_length=512):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(f\"Dataset: {file_path}\")\n",
    "        print(f\"Original dataset size:\\t{len(self.data)}\")\n",
    "\n",
    "        # self.data = self.data[self.data[\"text\"].apply(lambda x: len(tokenizer(format_input(x))[\"input_ids\"]) <= self.max_length)]\n",
    "        self.data = self.data[\n",
    "            self.data.apply(\n",
    "                lambda x: (\n",
    "                    len(tokenizer(format_input(x[\"text\"]))[\"input_ids\"]) <= self.max_length\n",
    "                    and len(tokenizer(x[\"phonemes\"])[\"input_ids\"]) <= self.max_length\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        print(f\"Reduced dataset size:\\t{len(self.data)}\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grapheme_text = self.data.iloc[idx][\"text\"]\n",
    "        phoneme_text = self.data.iloc[idx][\"phonemes\"]\n",
    "\n",
    "        # Force character-level tokenization\n",
    "        formatted_input = format_input(grapheme_text)\n",
    "\n",
    "        return formatted_input, phoneme_text\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    # Tokenize with dynamic padding (longest in batch)\n",
    "    input_enc = tokenizer(list(inputs), padding=True, return_tensors=\"pt\", truncation=False)\n",
    "    target_enc = tokenizer(list(targets), padding=True, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc.input_ids,\n",
    "        \"attention_mask\": input_enc.attention_mask,\n",
    "        \"labels\": target_enc.input_ids,\n",
    "    }\n",
    "\n",
    "# Load dataset and dataloader\n",
    "train_dataset = G2PDataset(\"clean_data/train-clean-100.csv\")\n",
    "val_dataset = G2PDataset(\"clean_data/test-clean.csv\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs, writer, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, use_cache=False)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            del input_ids, attention_mask, labels\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar(\"Training Loss\", avg_train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss, exact_match_accuracy, avg_per = validate_model(model, val_loader, writer)\n",
    "        writer.add_scalar(\"Validation Loss\", avg_val_loss, epoch)\n",
    "        writer.add_scalar(\"Exact Match\", exact_match_accuracy, epoch)\n",
    "        writer.add_scalar(\"Average PER\", avg_per, epoch)\n",
    "\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader, verbose=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_exact = 0\n",
    "    total_samples = 0\n",
    "    total_per = 0\n",
    "\n",
    "    progress_bar = tqdm(val_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "\n",
    "            # calculate per\n",
    "            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            pred_phonemes = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predicted_ids]\n",
    "            true_phonemes = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "\n",
    "            for pred, target in zip(pred_phonemes, true_phonemes):\n",
    "                if pred.strip() == target.strip():\n",
    "                    total_exact += 1\n",
    "                else:\n",
    "                    total_per += wer(target, pred)  # WER works similarly for phonemes\n",
    "\n",
    "            total_samples += len(batch['labels'])\n",
    "\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    exact_match_accuracy = total_exact / total_samples\n",
    "    avg_per = total_per / total_samples\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy (Exact Match): {exact_match_accuracy * 100:.2f}%\")\n",
    "        print(f\"Average Phoneme Error Rate (PER): {avg_per * 100:.2f}%\")\n",
    "\n",
    "    return avg_val_loss, exact_match_accuracy, avg_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, 10, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# Load the TensorBoard log directory\n",
    "ea = event_accumulator.EventAccumulator(\"logs/run_1\")\n",
    "ea.Reload()  # Load data from event files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"even in middle age they were still comely and the old grey haired women at their cottage doors had a dignity not to say majesty of their own\"\n",
    "# output = \"IY1 V IH0 N IH1 N M IH1 D AH0 L EY1 JH DH EY1 W ER0 S T IH1 L K AH1 M L IY0 sp AE1 N D DH IY0 OW1 L D G R EY1 HH EH1 R D W IH1 M AH0 N AE1 T DH EH1 R K AA1 T IH0 JH sp D AO1 R Z sp HH AE1 D AH0 D IH1 G N AH0 T IY0 sp N AA1 T T IH0 S EY1 M AE1 JH AH0 S T IY0 AH0 V DH EH1 R OW1 N sp\"\n",
    "# input = \"i expressed by signs my admiration and pleasure to my guides and they were greatly pleased\"\n",
    "# output = \"AY1 IH0 K S P R EH1 S T sp B AY1 S AY1 N Z M AY1 AE2 D M ER0 EY1 SH AH0 N AH0 N D P L EH1 ZH ER0 T AH0 M AY1 G AY1 D Z sp AE1 N D DH EY1 W ER0 G R EY1 T L IY0 P L IY1 Z D sp\"\n",
    "\n",
    "\n",
    "input_text = format_input(input)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, use_cache=True, max_length=1024)\n",
    "\n",
    "phoneme_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "phoneme_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

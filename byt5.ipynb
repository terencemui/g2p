{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 08:56:14.668790: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 08:56:14.716314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-01 08:56:15.651281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jiwer import wer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available and enabled\n",
      "Using CUDA with cuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "run_number = 0\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "train_dataset_path = \"clean_data/train-clean-100.csv\"\n",
    "val_dataset_path = \"clean_data/test-clean.csv\"\n",
    "\n",
    "model_name = \"google/byt5-small\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Cuda available and enabled\") if torch.cuda.is_available() else None\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using CUDA with cuDNN Enabled: {torch.backends.cudnn.enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "log_dir = f\"logs/run_{run_number}\"\n",
    "writer = SummaryWriter(log_dir=log_dir, purge_step=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: clean_data/train-clean-100.csv\n",
      "Original dataset size:\t28538\n",
      "Reduced dataset size:\t28538\n",
      "\n",
      "Dataset: clean_data/test-clean.csv\n",
      "Original dataset size:\t2620\n",
      "Reduced dataset size:\t2619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class\n",
    "class G2PDataset(Dataset):\n",
    "    def preprocess(self):\n",
    "        self.data[\"text\"] = \"grapheme to phoneme: \" + self.data[\"text\"]\n",
    "        self.data[\"len\"] = self.data[\"text\"].str.len() + 1\n",
    "        self.data = self.data[self.data[\"len\"] < self.max_length]\n",
    "\n",
    "    def __init__(self, file_path, max_length=512):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(f\"Dataset: {file_path}\")\n",
    "        print(f\"Original dataset size:\\t{len(self.data)}\")\n",
    "        self.preprocess()\n",
    "        print(f\"Reduced dataset size:\\t{len(self.data)}\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grapheme_text = self.data.iloc[idx][\"text\"]\n",
    "        phoneme_text = self.data.iloc[idx][\"phonemes\"]\n",
    "\n",
    "        return grapheme_text, phoneme_text\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    # Tokenize with dynamic padding (longest in batch)\n",
    "    input_enc = tokenizer(list(inputs), padding=True, return_tensors=\"pt\", truncation=False)\n",
    "    target_enc = tokenizer(list(targets), padding=True, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc.input_ids,\n",
    "        \"attention_mask\": input_enc.attention_mask,\n",
    "        \"labels\": target_enc.input_ids,\n",
    "    }\n",
    "\n",
    "train_dataset = G2PDataset(train_dataset_path)\n",
    "val_dataset = G2PDataset(val_dataset_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu_cache\n",
    "def clear_gpu_cache():\n",
    "    if device is torch.device(\"cuda\"):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.backends.cudnn.benchmark = False  # Reset benchmark tuning\n",
    "        torch.backends.cudnn.benchmark = True  # Re-enable after clearing\n",
    "    elif device is torch.device(\"mps\"):\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs, writer, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{start_epoch + epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, use_cache=False)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(\"Warning: NaN loss detected! Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            del input_ids, attention_mask, labels\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar(\"Training Loss\", avg_train_loss, epoch)\n",
    "        clear_gpu_cache()\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss, exact_match_accuracy, avg_per = validate_model(model, val_loader)\n",
    "        writer.add_scalar(\"Validation Loss\", avg_val_loss, epoch)\n",
    "        writer.add_scalar(\"Exact Match\", exact_match_accuracy, epoch)\n",
    "        writer.add_scalar(\"Average PER\", avg_per, epoch)\n",
    "\n",
    "        # save the model\n",
    "        model.save_pretrained(f\"weights/weights_{run_number}\")\n",
    "        torch.save(optimizer.state_dict(), f\"weights/weights_{run_number}/optimizer.pt\")\n",
    "        # model.save_pretrained(f\"weights/weights_{run_number}/epoch_{epoch}\")\n",
    "        # torch.save(optimizer.state_dict(), f\"weights/weights_{run_number}/epoch_{epoch}/optimizer.pt\")\n",
    "\n",
    "        clear_gpu_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader, verbose=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_exact = 0\n",
    "    total_samples = 0\n",
    "    total_per = 0\n",
    "\n",
    "    progress_bar = tqdm(val_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, use_cache=False)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "            # calculate per\n",
    "            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            pred_phonemes = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            true_phonemes = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            for pred, target in zip(pred_phonemes, true_phonemes):\n",
    "                if pred.strip() == target.strip():\n",
    "                    total_exact += 1\n",
    "                else:\n",
    "                    total_per += wer(target, pred)  # WER works similarly for phonemes\n",
    "\n",
    "            total_samples += len(batch['labels'])\n",
    "\n",
    "            clear_gpu_cache()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    exact_match_accuracy = total_exact / total_samples\n",
    "    avg_per = total_per / total_samples\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy (Exact Match): {exact_match_accuracy * 100:.2f}%\")\n",
    "        print(f\"Average Phoneme Error Rate (PER): {avg_per * 100:.2f}%\")\n",
    "\n",
    "    return avg_val_loss, exact_match_accuracy, avg_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 2/1784 [00:00<06:41,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 5/1784 [00:00<04:59,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 7/1784 [00:01<04:05,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 9/1784 [00:01<03:34,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 11/1784 [00:01<03:25,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 13/1784 [00:01<03:19,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 16/1784 [00:02<03:09,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 18/1784 [00:02<03:59,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 20/1784 [00:02<03:34,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 21/1784 [00:02<03:30,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏         | 24/1784 [00:03<03:15,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏         | 26/1784 [00:03<03:14,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 28/1784 [00:03<03:15,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 30/1784 [00:03<04:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 33/1784 [00:04<03:22,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 36/1784 [00:04<03:07,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 39/1784 [00:04<03:03,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 42/1784 [00:05<04:29,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 44/1784 [00:05<04:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 46/1784 [00:05<03:35,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 47/1784 [00:06<03:26,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 51/1784 [00:06<03:05,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 52/1784 [00:06<03:02,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 54/1784 [00:06<03:52,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 57/1784 [00:07<03:20,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 60/1784 [00:07<03:02,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 62/1784 [00:07<02:57,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▎         | 65/1784 [00:08<02:55,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▍         | 67/1784 [00:08<03:44,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▍         | 69/1784 [00:08<03:29,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▍         | 71/1784 [00:08<03:12,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▍         | 74/1784 [00:09<02:59,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▍         | 76/1784 [00:09<03:31,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN loss detected! Skipping batch.\n",
      "Warning: NaN loss detected! Skipping batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, writer, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(loss):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: NaN loss detected! Skipping batch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# optimizer.zero_grad()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs, writer)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
